import os
import types
import csv
from typing import List
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor
from torch_geometric.data import Data
from torch_geometric.loader import NeighborLoader
from methods.base import FraudDetection
from utils.utils import (
    evaluate,
    get_class_weights,
    split_graph_private_public_pyg,
)
from modules.teacher import TeacherModule
from modules.student import Grace, Infomax, StudentModel, GraphMAE
from modules.gnn_encoder import GNNEncoder
from modules.aggregator import ABTEModule, Aggregator
from trainer import Metrics, Trainer
from sklearn.metrics import roc_auc_score
import GCL.augmentors as A
import GCL.losses as L
from GCL.models import DualBranchContrast
from models.sub_graph import CugraphOverlapPartitioner

augmentor_er = A.EdgeRemoving(pe=0.3)
augmentor_fm = A.FeatureMasking(pf=0.3)


class NADES(FraudDetection):

    def __init__(
        self,
        detection_type: str,
        epochs: int,
        num_neighbor: int,
        num_layers: int,
        batch_size: int,
        lr: float,
        in_channels: int,
        out_channels: int,
        hidden_channels: int,
        dropout: float,
        logger,
        epsilon: float,
        delta: float,
        max_grad_norm: float,
        patience: int,
        privacy_ratio,
        # Partition method
        partition_method,  # å•ä¸€åˆ’åˆ†æ–¹æ³•ï¼Œé»˜è®¤ "D"
        num_partitions,
        max_overlap_per_node,
        # Teacher
        teacher_epochs,
        teacher_patience,
        # Aggregator
        num_queries,  # 200, 400, 1000
        consistency_weight,  # 1.0, 5.0, 10.0
        ssl_method,  # "infomax", "grace", "graphmae","none"
        # Student
        ssl_epochs: int = 500,
        ssl_patience: int = 20,
        ssl_lr: float = 0.005,
        ssl_grace_tau: float = 0.2,
        dirichlet_gamma: float = 1.0,
        sup_weight: float = 1.0,
        Delta_p_bound: float = 2.0,
        Delta_alpha_bound: float = 1.0,
        evaluate_abte_upper_bound: bool = False,
        metadata=None,
        cache_dir: str = None,  # ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨å†…ç½®è·¯å¾„

        **kwargs,  # Accept any other parameters for compatibility
    ):
        super().__init__(
            detection_type,
            epochs,
            num_neighbor,
            num_layers,
            batch_size,
            lr,
            logger,
            patience,
        )

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.hidden_channels = hidden_channels
        self.num_layers = num_layers
        self.dropout = dropout
        self.delta = delta
        self.dataset = metadata
        self.privacy_ratio = privacy_ratio
        # Partition settings
        self.partition_method = partition_method
        self.S = num_partitions
        self.s_max = max_overlap_per_node
        self.partitioner = CugraphOverlapPartitioner(
            device=self.device,
            train_ratio=0.8,
            val_ratio=0.2,
        )
        # Teacher settings
        self.teacher_epochs = teacher_epochs
        self.teacher_patience = teacher_patience
        self.max_grad_norm = max_grad_norm
        # Aggregator settings
        self.num_queries = num_queries
        self.eta = epsilon
        self.gamma = dirichlet_gamma
        self.Delta_p_bound = Delta_p_bound
        self.Delta_alpha_bound = Delta_alpha_bound
        self.fraud_class_idx = 1 if out_channels > 1 else 0
        self.evaluate_abte_upper_bound = evaluate_abte_upper_bound
        # Cache settings
        if cache_dir is None:
            self.cache_dir = "/home/workspace/Dataset/PDP-GKD"
        else:
            self.cache_dir = cache_dir
        os.makedirs(self.cache_dir, exist_ok=True)
        private_data_name = f"{self.dataset}_private_data.pt"
        public_data_name = f"{self.dataset}_public_data.pt"
        self.private_data_path = os.path.join(
            self.cache_dir, private_data_name)
        self.public_data_path = os.path.join(self.cache_dir, public_data_name)

        self.ssl_epochs = ssl_epochs
        self.ssl_patience = ssl_patience
        self.ssl_lr = ssl_lr
        self.ssl_grace_tau = ssl_grace_tau
        self.info = {}
        self.ssl_method = ssl_method.lower()
        if self.ssl_method not in {"infomax", "grace", "graphmae", "none"}:
            raise ValueError(
                f"Unsupported ssl_method '{ssl_method}'. Expected 'infomax', 'grace', 'graphmae', or 'none'."
            )
        self.finetune_patience = patience
        self.lambda_sup = sup_weight
        self.lambda_con = consistency_weight

        self.student_encoder: GNNEncoder = GNNEncoder(
            in_channels=self.in_channels,
            hidden_channels=self.hidden_channels,
            num_layers=self.num_layers,
            dropout=self.dropout,
        ).to(self.device)
        self.student: StudentModel = StudentModel(
            encoder=self.student_encoder,
            hidden_channels=self.hidden_channels,
            out_channels=self.out_channels,
            dropout=self.dropout,
        ).to(self.device)

        # State
        self.private_data: Data | None = None
        self.public_data: Data | None = None
        self.partitions: List[List[int]] | None = None
        self.teachers: List[TeacherModule] = []
        # [num_teachers, num_buckets, 6] for bucket-conditioned TSV
        self.tsv: torch.Tensor | None = None
        self.bucket_thresholds: dict | None = None  # å­˜å‚¨åˆ†æ¡¶é˜ˆå€¼ {tau_deg, tau_fea}
        self.teacher_predictions: torch.Tensor | None = None  # [N_pub, S, C]

    @property
    def detector(self) -> nn.Module:
        return self.student

    def fit(self, data: Data, prefix: str = "nades/") -> nn.Module:
        self.pre_fit(data)
        self.logger.info("(1) Prepare private and public data")
        self._prepare_private_public()

        self.logger.info(f"(2) ä½¿ç”¨{self.partition_method}æ–¹æ³•è¿›è¡Œå›¾åˆ’åˆ†å’Œå­å›¾æå–")

        # æ„å»ºç¼“å­˜è·¯å¾„
        # partition_subgraphs_path = os.path.join(
        # self.cache_dir, f"partition_{self.partition_method}_subgraphs.pt"
        # )
        partition_subgraphs_path = None
        # tsv_path = os.path.join(self.cache_dir, f"tsv_{self.partition_method}.pt")
        tsv_path = None
        # è°ƒç”¨å•ä¸€åˆ’åˆ†æ–¹æ³•
        self.subgraphs = self.partitioner.partition(
            method=self.partition_method,
            data=self.private_data,
            S=self.S,
            s_max=self.s_max,
            resolution=1.0,
            cache_path=partition_subgraphs_path,
            logger=self.logger,
        )

        self.info["partition_method"] = self.partition_method
        self.info["num_subgraphs"] = len(self.subgraphs)
        self.info["max_overlap_per_node"] = self.s_max
        self.logger.info(
            f"åˆ’åˆ†å®Œæˆ: æ–¹æ³•: {self.partition_method}, å­å›¾æ•°é‡: {len(self.subgraphs)}, s_max: {self.s_max}"
        )

        self.logger.info("(3) Train teachers with Trainer")
        self.teachers = self._train_teachers_with_trainer(self.subgraphs)

        # è®¡ç®—æ¡¶æ¡ä»¶åŒ–TSVï¼ˆåœ¨æ•™å¸ˆè®­ç»ƒåï¼‰
        self.logger.info("(3.5) Compute bucket-conditioned TSV")
        self.tsv, self.bucket_thresholds = self._compute_bucket_conditioned_tsv(
            self.public_data,
            self.teachers,
            num_buckets=4,
            anchor_ratio=0.1,  # ä½¿ç”¨10%çš„å…¬å…±èŠ‚ç‚¹ä½œä¸ºé”šç‚¹
            logger=self.logger,
        )

        # è®¡ç®— TSV åæ¸…ç†å†…å­˜
        torch.cuda.empty_cache()

        self.logger.info("(4) Student SSL pretraining on public")
        self._run_ssl_with_trainer_monkey(self.public_data)
        self.logger.info("SSL pretraining completed")

        self.logger.info("(5) Generate privacy-preserving pseudo labels")
        self._generate_pseudo_labels()
        # self._generate_pseudo_labels_mean()
        self.logger.info("Pseudo label generation completed")

        self.logger.info("(6) Semi-supervised fine-tuning with pseudo labels")
        self._run_finetune_with_trainer_monkey(self.public_data)
        self.logger.info("Semi-supervised finetune completed")

        self.logger.info(self.info)
        self._save_info_to_csv()

        return self.student

    def data_loader(self, data: Data, stage, num_neighbors: List[int] = None):
        """Return a dataloader for the given stage."""
        if num_neighbors is None:
            num_neighbors = [self.num_neighbor] * (self.num_layers + 1)
        input_nodes = (
            data.train_mask
            if stage == "train"
            else data.val_mask
            if stage == "val"
            else data.test_mask
        )
        dataloader = NeighborLoader(
            data,
            num_neighbors=num_neighbors,
            batch_size=self.batch_size,
            shuffle=(stage == "train"),
            input_nodes=input_nodes,
        )

        return dataloader

    def _save_info_to_csv(self) -> None:
        if not self.info:
            self.logger.warning("self.info ä¸ºç©ºï¼Œè·³è¿‡å†™å…¥ CSV")
            return
        csv_path = os.path.join(self.cache_dir, "pdp_gkd_info.csv")
        try:
            flat_info = self._flatten_info(self.info)
            if not flat_info:
                self.logger.warning("å±•å¼€åçš„ self.info ä¸ºç©ºï¼Œè·³è¿‡å†™å…¥ CSV")
                return
            existing_fieldnames: List[str] = []
            existing_rows: List[dict] = []
            if os.path.exists(csv_path):
                with open(csv_path, newline="", encoding="utf-8") as csvfile:
                    reader = csv.DictReader(csvfile)
                    if reader.fieldnames:
                        existing_fieldnames = list(reader.fieldnames)
                        existing_rows = list(reader)
            fieldnames = existing_fieldnames.copy()
            for key in flat_info.keys():
                if key not in fieldnames:
                    fieldnames.append(key)
            for row in existing_rows:
                for key in fieldnames:
                    row.setdefault(key, "")
            new_row = {key: flat_info.get(key, "") for key in fieldnames}
            existing_rows.append(new_row)
            with open(csv_path, "w", newline="", encoding="utf-8") as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(existing_rows)
            self.logger.info(f"self.info å·²å†™å…¥ {csv_path}")
        except OSError as exc:
            self.logger.error(f"å†™å…¥ self.info CSV å¤±è´¥: {exc}")

    def _flatten_info(self, info: dict) -> dict:
        flat: dict = {}

        def _flatten_item(key: str, value):
            if isinstance(value, dict):
                for sub_key, sub_value in value.items():
                    _flatten_item(sub_key, sub_value)
            else:
                flat[key] = self._normalize_value(value)

        for key, value in info.items():
            _flatten_item(key, value)
        return flat

    @staticmethod
    def _normalize_value(value):
        if isinstance(value, (list, tuple, set)):
            return ",".join(map(str, value))
        return value

    def _train_teachers_with_trainer(
        self, subgraphs: List[Data]
    ) -> List[TeacherModule]:
        teachers: List[TeacherModule] = []
        for i, sub in enumerate(subgraphs):
            # å†æ¬¡éªŒè¯å­å›¾æœ‰æ•ˆæ€§
            if sub.num_nodes < 2:
                self.logger.warning(
                    f"è·³è¿‡ Teacher {i + 1}ï¼šèŠ‚ç‚¹æ•°è¿‡å°‘ ({sub.num_nodes})"
                )
                continue
            if sub.edge_index.size(1) == 0:
                self.logger.warning(f"è·³è¿‡ Teacher {i + 1}ï¼šæ²¡æœ‰è¾¹")
                continue
            if not hasattr(sub, "train_mask") or not sub.train_mask.any():
                self.logger.warning(f"è·³è¿‡ Teacher {i + 1}ï¼šæ²¡æœ‰è®­ç»ƒèŠ‚ç‚¹")
                continue
            # æ£€æŸ¥è®­ç»ƒé›†ä¸­æ˜¯å¦æœ‰å¤šä¸ªç±»åˆ«
            train_labels = sub.y[sub.train_mask]
            unique_labels = torch.unique(train_labels)
            if len(unique_labels) < 2:
                self.logger.warning(
                    f"è·³è¿‡ Teacher {i + 1}ï¼šè®­ç»ƒé›†åªæœ‰å•ä¸€ç±»åˆ« (ç±»åˆ«={unique_labels.tolist()})"
                )
                continue

            self.logger.info(f"Teacher {i + 1}/{len(subgraphs)} training:")

            teacher = TeacherModule(
                in_channels=self.in_channels,
                hidden_channels=self.hidden_channels,
                out_channels=self.out_channels,
                num_layers=self.num_layers,
                dropout=self.dropout,
            ).to(self.device)
            trainer_t = Trainer(
                logger=self.logger,
                patience=self.teacher_patience,
                monitor="val/auc",
                monitor_mode="max",
            )
            optimizer = torch.optim.Adam(teacher.parameters(), lr=self.lr)

            # çŒ´å­è¡¥ä¸ï¼šæ›¿æ¢ Trainer.step æ–¹æ³•ï¼Œåœ¨ backward ä¹‹åã€optimizer.step() ä¹‹å‰æ·»åŠ æ¢¯åº¦è£å‰ª
            # ä½¿ç”¨é»˜è®¤å‚æ•°æ•è·å¾ªç¯å˜é‡ï¼Œé¿å…é—­åŒ…é—®é¢˜
            def teacher_step(
                tr, batch, stage, prefix, teacher_idx=i, subgraph=sub
            ) -> Metrics:
                if stage == "train":
                    tr.optimizer.zero_grad(set_to_none=True)

                grad_state = torch.is_grad_enabled()
                torch.set_grad_enabled(stage == "train")

                # è°ƒç”¨ model.step
                loss, logits_masked, y = teacher.step(
                    batch,
                    stage=stage,
                    global_weights=tr.global_weights,
                )

                torch.set_grad_enabled(grad_state)

                if stage == "train" and loss is not None:
                    # æ£€æŸ¥ loss æ˜¯å¦ä¸º NaN æˆ– Inf
                    if torch.isnan(loss) or torch.isinf(loss):
                        self.logger.error(
                            f"Teacher {teacher_idx + 1} è®­ç»ƒæ—¶å‡ºç°æ— æ•ˆæŸå¤±å€¼: {loss.item()}, "
                            f"å­å›¾èŠ‚ç‚¹æ•°={subgraph.num_nodes}, è®­ç»ƒèŠ‚ç‚¹æ•°={subgraph.train_mask.sum().item()}"
                        )
                        # è·³è¿‡è¿™ä¸ª batchï¼Œä¸è¿›è¡Œåå‘ä¼ æ’­
                        return logits_masked, y, loss

                    with torch.autograd.set_detect_anomaly(True):
                        loss.backward()
                        # æ¢¯åº¦è£å‰ªï¼šåœ¨ backward ä¹‹åã€step ä¹‹å‰
                        torch.nn.utils.clip_grad_norm_(
                            teacher.parameters(), max_norm=self.max_grad_norm
                        )
                        tr.optimizer.step()

                return logits_masked, y, loss

            trainer_t.step = types.MethodType(teacher_step, trainer_t)

            # å•å­å›¾ä½œä¸º dataloaderï¼›mask å†…å·²å« train/val
            trainer_t.fit(
                model=teacher,
                epochs=self.teacher_epochs,
                optimizer=optimizer,
                # train_dataloader=self.data_loader(sub, "train"),
                train_dataloader=[sub],
                # val_dataloader=self.data_loader(sub, "val"),
                val_dataloader=[sub],
                test_dataloader=None,
                checkpoint=True,
                global_weights=get_class_weights(sub).to(self.device),
                prefix="teacher/",
            )
            teachers.append(teacher)

            # æ¸…ç† trainer å’Œ optimizer ä»¥é‡Šæ”¾å†…å­˜
            del trainer_t, optimizer
            torch.cuda.empty_cache()

        return teachers

    def _run_ssl_with_trainer_monkey(self, pub: Data) -> None:
        """è·¯ç”±åˆ°å¯¹åº”çš„ SSL è®­ç»ƒæ–¹æ³•"""
        self.info["ssl_method"] = self.ssl_method

        method_map = {
            "infomax": self._ssl_train_infomax,
            "grace": self._ssl_train_grace,
            "graphmae": self._ssl_train_graphmae,
            "none": self._skip_ssl_pretraining,
        }

        if self.ssl_method not in method_map:
            raise ValueError(
                f"Unsupported ssl_method '{self.ssl_method}'. "
                f"Expected one of {list(method_map.keys())}."
            )

        method_map[self.ssl_method](pub)

    def _skip_ssl_pretraining(self, pub: Data) -> None:
        """
        æ¶ˆèå®éªŒï¼šè·³è¿‡SSLé¢„è®­ç»ƒï¼Œç¼–ç å™¨ä¿æŒéšæœºåˆå§‹åŒ–çŠ¶æ€
        åœ¨ç”Ÿæˆä¼ªæ ‡ç­¾æ—¶ï¼ŒNHVï¼ˆèŠ‚ç‚¹éšè—å‘é‡ï¼‰å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„ç¼–ç å™¨å¯¹èŠ‚ç‚¹ç‰¹å¾è¿›è¡Œç¼–ç 
        """
        self.info["ssl_method"] = "none (random initialization)"
        self.logger.info(
            "è·³è¿‡SSLé¢„è®­ç»ƒï¼šç¼–ç å™¨ä¿æŒéšæœºåˆå§‹åŒ–çŠ¶æ€ï¼Œ"
            "å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„ç¼–ç å™¨å¯¹èŠ‚ç‚¹ç‰¹å¾è¿›è¡Œç¼–ç ä»¥ç”ŸæˆNHV"
        )
        # ç¡®ä¿ç¼–ç å™¨åœ¨è¯„ä¼°æ¨¡å¼ä¸‹ï¼ˆè™½ç„¶å‚æ•°æ˜¯éšæœºçš„ï¼‰
        self.student_encoder.eval()
        self.student.eval()

    def _run_ssl_training(
        self,
        model: nn.Module,
        loss_fn,
        train_dataloader,
        val_dataloader,
        scheduler_t_max: int = None,
    ) -> dict:
        """è¿è¡Œ SSL è®­ç»ƒçš„é€šç”¨æµç¨‹"""
        device = self.device
        model = model.to(device)

        trainer_ssl = Trainer(
            logger=self.logger,
            patience=self.ssl_patience,
            monitor="val/loss",
            monitor_mode="min",
        )

        def ssl_loop(tr, dataloader, stage: str, prefix: str):
            model = tr.model
            model.train(stage == "train")
            total_loss = 0.0
            steps = 0

            for batch in dataloader:
                if stage == "train":
                    tr.optimizer.zero_grad(set_to_none=True)

                grad_state = torch.is_grad_enabled()
                torch.set_grad_enabled(stage == "train")

                batch = batch.to(device)
                loss = loss_fn(model, batch, tr)

                torch.set_grad_enabled(grad_state)

                if stage == "train" and loss is not None:
                    loss.backward()
                    tr.optimizer.step()
                    if hasattr(tr, "scheduler") and tr.scheduler is not None:
                        tr.scheduler.step()

                total_loss += float(loss.detach().cpu())
                steps += 1

            avg_loss = total_loss / steps if steps > 0 else 0.0
            return {f"{prefix}{stage}/loss": avg_loss}

        if scheduler_t_max is None:
            scheduler_t_max = self.ssl_epochs

        optimizer = torch.optim.Adam(model.parameters(), lr=self.ssl_lr)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=scheduler_t_max, eta_min=1e-5
        )

        orig_loop = trainer_ssl.loop
        try:
            trainer_ssl.loop = types.MethodType(ssl_loop, trainer_ssl)
            trainer_ssl.optimizer = optimizer
            trainer_ssl.scheduler = scheduler

            best_metrics = trainer_ssl.fit(
                model=model,
                epochs=self.ssl_epochs,
                optimizer=optimizer,
                train_dataloader=train_dataloader,
                val_dataloader=val_dataloader,
                test_dataloader=None,
                checkpoint=True,
                global_weights=None,
                prefix="ssl/",
            )
            return best_metrics
        finally:
            trainer_ssl.loop = orig_loop

    def _ssl_train_infomax(self, pub: Data) -> None:
        """è®­ç»ƒ Infomax SSL æ¨¡å‹"""
        infomax_model = Infomax(
            encoder=self.student_encoder,
            hidden_channels=self.hidden_channels,
        )

        def loss_fn(model, batch, tr):
            pos_z, neg_z, summary_vec = model(batch.x, batch.edge_index)
            return model.loss(pos_z, neg_z, summary_vec)

        best_metrics = self._run_ssl_training(
            model=infomax_model,
            loss_fn=loss_fn,
            train_dataloader=[pub],
            val_dataloader=[pub],
        )
        self.info["ssl_best_metrics"] = best_metrics

    def _ssl_train_grace(self, pub: Data) -> None:
        """è®­ç»ƒ GRACE SSL æ¨¡å‹"""
        device = self.device

        contrast_model = DualBranchContrast(
            loss=L.InfoNCE(tau=self.ssl_grace_tau), mode="L2L", intraview_negs=False
        ).to(device)

        grace_model = Grace(
            encoder=self.student_encoder,
            hidden_channels=self.hidden_channels,
            projection_dim=self.hidden_channels,
        )

        def loss_fn(model, batch, tr):
            x_1, edge_index_1, _ = augmentor_er(
                batch.x, batch.edge_index, edge_weight=None
            )
            x_2, edge_index_2, _ = augmentor_fm(
                batch.x, batch.edge_index, edge_weight=None
            )
            z_1 = model(x_1, edge_index_1)
            z_2 = model(x_2, edge_index_2)
            h_1, h_2 = [model.projector(z) for z in (z_1, z_2)]
            return contrast_model(h_1, h_2)

        train_loader = self.data_loader(pub, "train")
        val_loader = self.data_loader(pub, "val")
        steps_per_epoch = len(train_loader)
        scheduler_t_max = (
            steps_per_epoch * self.ssl_epochs
            if steps_per_epoch > 0
            else self.ssl_epochs
        )

        best_metrics = self._run_ssl_training(
            model=grace_model,
            loss_fn=loss_fn,
            train_dataloader=train_loader,
            val_dataloader=val_loader,
            scheduler_t_max=scheduler_t_max,
        )
        self.info["ssl_best_metrics"] = best_metrics

    def _ssl_train_graphmae(self, pub: Data) -> None:
        """è®­ç»ƒ GraphMAE SSL æ¨¡å‹"""
        device = self.device

        graphmae = GraphMAE(
            encoder=self.student_encoder,
            in_channels=self.in_channels,
            out_channels=self.hidden_channels,
            mask_rate=0.5,
            decoder_dim=self.hidden_channels,
            replace_with_zero=False,
        )

        def loss_fn(model, batch, tr):
            x_recon, x_orig, mask_idx = model(
                batch.x.to(device), batch.edge_index.to(device)
            )
            return model.loss(x_recon, x_orig, mask_idx)

        data = pub.to(device)
        best_metrics = self._run_ssl_training(
            model=graphmae,
            loss_fn=loss_fn,
            train_dataloader=[data],
            val_dataloader=[data],
        )
        self.info["ssl_best_metrics"] = best_metrics

    def _generate_pseudo_labels(self) -> None:
        """
        åˆå§‹åŒ–èšåˆå™¨ï¼ˆABTEæ¨¡å—å’ŒAggregatorï¼‰ã€é¢„è®¡ç®—ç¼“å­˜ï¼Œå¹¶ç”Ÿæˆéšç§ä¿æŠ¤çš„ä¼ªæ ‡ç­¾
        åŒ…æ‹¬ï¼š
        1. åˆå§‹åŒ–ABTEæ¨¡å—å’Œèšåˆå™¨
        2. é¢„è®¡ç®—æ•™å¸ˆé¢„æµ‹ç¼“å­˜å’ŒèŠ‚ç‚¹ç»Ÿè®¡ç¼“å­˜
        3. ç”Ÿæˆéšç§ä¿æŠ¤çš„ä¼ªæ ‡ç­¾å¹¶å­˜å‚¨åˆ°public_data
        """
        # åˆå§‹åŒ–ABTEæ¨¡å—
        # TSVå½¢çŠ¶: [num_teachers, num_buckets, 6] æˆ– [num_teachers, 6]
        if self.tsv.dim() == 3:
            tsv_dim = self.tsv.shape[2]  # æ¡¶æ¡ä»¶åŒ–TSV
        else:
            tsv_dim = self.tsv.shape[1]  # éæ¡¶æ¡ä»¶åŒ–TSV

        self.attn = ABTEModule(
            tsv_dim=tsv_dim,
            node_emb_dim=self.hidden_channels,
            node_stats_dim=2,  # NHVåŒ…å« [log(1+deg), ||x_v||_2]ï¼Œå…±2ä¸ªç»Ÿè®¡ç‰¹å¾
            num_teachers=len(self.teachers),
            num_classes=self.out_channels,
            hidden_dim=self.hidden_channels,
        ).to(self.device)

        # è®¾ç½®éšç§å‚æ•°

        self.delta_1_F = self.s_max * \
            (self.Delta_p_bound + self.Delta_alpha_bound)

        # åˆå§‹åŒ–èšåˆå™¨
        self.aggregator = Aggregator(
            abte_module=self.attn,
            tsv=self.tsv,
            eta=self.eta,
            delta=self.delta,
            gamma_dirichlet=self.gamma,
            delta_1_F_sensitivity=self.delta_1_F,
        )

        # é¢„è®¡ç®—æ•™å¸ˆé¢„æµ‹ä¸ºæ¦‚ç‡æ ¼å¼ [N, S, num_classes]ï¼Œå…¶ä¸­Sæ˜¯æ•™å¸ˆæ•°é‡
        with torch.no_grad():
            self.teacher_preds_cache = self._predict_public_by_teachers(
                self.public_data, self.teachers
            )
            self.public_data.teacher_preds_cache = self.teacher_preds_cache

        # é¢„è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹çš„ç»Ÿè®¡ç‰¹å¾ [N, stats_dim]
        self.node_stats_cache = self._compute_node_stats(self.public_data).to(
            self.device
        )
        self.public_data.node_stats = self.node_stats_cache

        # ç”Ÿæˆéšç§ä¿æŠ¤çš„ä¼ªæ ‡ç­¾
        self.pseudo_labels_dict = self._query_pseudo_labels_random_abte(
            self.public_data
        )
        self.logger.info(
            f"Has pseudo and true label: {sum(self.public_data.y[self.public_data.has_pseudo_label] == 0)}: normal, {sum(self.public_data.y[self.public_data.has_pseudo_label] == 1)}: fraud"
        )
        if self.evaluate_abte_upper_bound:
            self._evaluate_abte_upper_bound()

    def _generate_pseudo_labels_mean(self) -> None:
        """
        æ¶ˆèå®éªŒï¼šä½¿ç”¨æ•™å¸ˆé¢„æµ‹çš„å¹³å‡å€¼ç”Ÿæˆä¼ªæ ‡ç­¾ï¼ˆä¸ä½¿ç”¨ABTEï¼‰
        åŒ…æ‹¬ï¼š
        1. é¢„è®¡ç®—æ•™å¸ˆé¢„æµ‹ç¼“å­˜
        2. é¢„è®¡ç®—èŠ‚ç‚¹ç»Ÿè®¡ç¼“å­˜ï¼ˆè™½ç„¶ä¸ä½¿ç”¨ï¼Œä½†ä¿æŒä¸€è‡´æ€§ï¼‰
        3. ç”ŸæˆåŸºäºæ•™å¸ˆå¹³å‡é¢„æµ‹çš„ä¼ªæ ‡ç­¾å¹¶å­˜å‚¨åˆ°public_data
        """
        # è®¾ç½®éšç§å‚æ•°
        self.delta_1_F = self.s_max * \
            (self.Delta_p_bound + self.Delta_alpha_bound)

        # åˆå§‹åŒ–èšåˆå™¨ï¼ˆä»…ç”¨äºåŠ å™ªï¼Œä¸éœ€è¦ABTEæ¨¡å—ï¼‰
        # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„ABTEæ¨¡å—ï¼ˆä¸ä¼šè¢«ä½¿ç”¨ï¼‰
        dummy_abte = ABTEModule(
            tsv_dim=self.tsv.shape[1],
            node_emb_dim=self.hidden_channels,
            node_stats_dim=2,  # NHVåŒ…å« [log(1+deg), ||x_v||_2]ï¼Œå…±2ä¸ªç»Ÿè®¡ç‰¹å¾
            num_teachers=len(self.teachers),
            num_classes=self.out_channels,
            hidden_dim=self.hidden_channels,
        ).to(self.device)

        self.aggregator = Aggregator(
            abte_module=dummy_abte,
            tsv=self.tsv,
            eta=self.eta,
            delta=self.delta,
            gamma_dirichlet=self.gamma,
            delta_1_F_sensitivity=self.delta_1_F,
        )

        # é¢„è®¡ç®—æ•™å¸ˆé¢„æµ‹ä¸ºæ¦‚ç‡æ ¼å¼ [N, S, num_classes]ï¼Œå…¶ä¸­Sæ˜¯æ•™å¸ˆæ•°é‡
        with torch.no_grad():
            self.teacher_preds_cache = self._predict_public_by_teachers(
                self.public_data, self.teachers
            )
            self.public_data.teacher_preds_cache = self.teacher_preds_cache

        # é¢„è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹çš„ç»Ÿè®¡ç‰¹å¾ [N, stats_dim]ï¼ˆè™½ç„¶ä¸ä½¿ç”¨ï¼Œä½†ä¿æŒä¸€è‡´æ€§ï¼‰
        self.node_stats_cache = self._compute_node_stats(self.public_data).to(
            self.device
        )
        self.public_data.node_stats = self.node_stats_cache

        # ç”ŸæˆåŸºäºæ•™å¸ˆå¹³å‡é¢„æµ‹çš„ä¼ªæ ‡ç­¾
        self.pseudo_labels_dict = self._query_pseudo_labels_mean(
            self.public_data)
        self.logger.info(
            f"Has pseudo and true label: {sum(self.public_data.y[self.public_data.has_pseudo_label] == 0)}: normal, {sum(self.public_data.y[self.public_data.has_pseudo_label] == 1)}: fraud"
        )

    def _run_finetune_with_trainer_monkey(self, pub: Data) -> None:
        trainer_ft = Trainer(
            logger=self.logger,
            patience=self.finetune_patience,
            monitor="val/auc",
            monitor_mode="max",
        )
        self.info["finetune_patience"] = self.finetune_patience
        # self.info["lambda_sup"] = self.lambda_sup
        # self.info["lambda_con"] = self.lambda_con

        def finetune_loop(tr, dataloader, stage: str, prefix: str) -> Metrics:
            # model æ˜¯ StudentModel
            model = tr.model
            device = next(model.parameters()).device
            logits_list = []
            y_list = []

            def finetune_step(tr, batch, stage, prefix):
                if stage == "train":
                    model.train()
                    tr.optimizer.zero_grad(set_to_none=True)
                grad_state = torch.is_grad_enabled()
                torch.set_grad_enabled(stage == "train")
                # --- 1. å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­ ---
                #  model åªåœ¨å°æ‰¹æ¬¡ 'batch' ä¸Šè¿è¡Œ
                pi_v_all_batch, _ = model(
                    batch.x, batch.edge_index, return_intermediate=True
                )

                # å–å‡º"ä¸­å¿ƒ"èŠ‚ç‚¹çš„ç»“æœï¼ˆåªå–æœ‰æ•ˆçš„è¾“å…¥èŠ‚ç‚¹ï¼‰
                valid_batch_size = B_t_indices.numel()
                log_pi_B = pi_v_all_batch[:valid_batch_size]
                y_B = batch.y[:valid_batch_size]

                # --- 2. è®¡ç®—ç›‘ç£æŸå¤± L_sup ---
                # åªå¯¹æœ‰ä¼ªæ ‡ç­¾çš„èŠ‚ç‚¹è®¡ç®—ç›‘ç£æŸå¤±
                sup_loss = None
                if (
                    stage == "train"
                    and hasattr(self.public_data, "has_pseudo_label")
                    and self.public_data.has_pseudo_label is not None
                ):
                    # è·å–å½“å‰æ‰¹æ¬¡ä¸­æœ‰ä¼ªæ ‡ç­¾çš„èŠ‚ç‚¹
                    batch_has_pseudo = self.public_data.has_pseudo_label[
                        B_t_indices
                    ]  # [B]

                    if batch_has_pseudo.any():
                        # è·å–æœ‰ä¼ªæ ‡ç­¾çš„èŠ‚ç‚¹çš„é¢„æµ‹å’Œä¼ªæ ‡ç­¾
                        pseudo_labels_B = self.public_data.pseudo_labels[
                            B_t_indices
                        ]  # [B, K]
                        # [B_pseudo, K]
                        log_pi_pseudo = log_pi_B[batch_has_pseudo]
                        pseudo_labels_pseudo = pseudo_labels_B[
                            batch_has_pseudo
                        ]  # [B_pseudo, K]

                        # è®¡ç®—åŒå‘KLæ•£åº¦
                        sup_loss_1 = F.kl_div(
                            log_pi_pseudo,
                            pseudo_labels_pseudo,
                            reduction="batchmean",
                            log_target=False,
                        )
                        sup_loss_2 = F.kl_div(
                            F.log_softmax(pseudo_labels_pseudo, dim=-1),
                            log_pi_pseudo.exp(),
                            reduction="batchmean",
                            log_target=False,
                        )
                        sup_loss = (sup_loss_1 + sup_loss_2) / 2
                    else:
                        # å¦‚æœæ²¡æœ‰ä¼ªæ ‡ç­¾èŠ‚ç‚¹ï¼Œç›‘ç£æŸå¤±ä¸º0
                        sup_loss = torch.tensor(
                            0.0, device=device, requires_grad=True)

                # --- 3. è®¡ç®— L_con ---
                with torch.no_grad():
                    pi = log_pi_B.exp()  # å¤ç”¨è®¡ç®—

                #  å¿…é¡»å¯¹ *åŒä¸€ä¸ªæ‰¹æ¬¡* è¿›è¡Œå¢å¼º
                aug_x, aug_edge_index, _ = augmentor_er(
                    batch.x, batch.edge_index, edge_weight=None
                )
                aug_batch_data = batch.clone()
                aug_batch_data.x = aug_x
                aug_batch_data.edge_index = aug_edge_index

                log_pi_prime_batch, _ = model(
                    aug_batch_data.x,
                    aug_batch_data.edge_index,
                    return_intermediate=True,
                )
                log_pi_prime_B = log_pi_prime_batch[:valid_batch_size]

                con_loss_1 = F.kl_div(
                    log_pi_prime_B, pi, reduction="batchmean", log_target=False
                )
                con_loss_2 = F.kl_div(
                    pi.log(),
                    log_pi_prime_B.exp(),
                    reduction="batchmean",
                    log_target=False,
                )
                con_loss = (con_loss_1 + con_loss_2) / 2

                # --- 4. æ€»æŸå¤±ä¸æ›´æ–° ---
                if sup_loss is not None:
                    loss = self.lambda_sup * sup_loss + self.lambda_con * con_loss
                else:
                    loss = self.lambda_con * con_loss

                torch.set_grad_enabled(grad_state)
                if stage == "train" and loss is not None:
                    with torch.autograd.set_detect_anomaly(True):
                        loss.backward()
                        tr.optimizer.step()

                return log_pi_B.detach(), y_B.detach()

            for batch in dataloader:
                batch = batch.to(device)
                B_t_indices = batch.input_id  # å…¨å±€ç´¢å¼•
                logits, y = finetune_step(tr, batch, stage, prefix)
                # logits, y = finetune_step_privacy(tr, batch, stage, prefix)
                logits_list.append(logits)
                y_list.append(y)
            logits = torch.cat(logits_list, dim=0)
            y = torch.cat(y_list, dim=0).cpu()

            label_mask = y != -1
            if label_mask.any():
                logits = logits[label_mask]
                y = y[label_mask]

            if stage == "test":
                result = evaluate(y, logits)
                return {
                    f"{prefix}{stage}/{key}": value for key, value in result.items()
                }
            else:
                probs = F.softmax(logits, dim=1)[:, 1].cpu().detach().numpy()
                auc = roc_auc_score(y, probs)
                loss = F.cross_entropy(logits.cpu(), y)
                return {
                    f"{prefix}{stage}/auc": auc,
                    f"{prefix}{stage}/loss": loss.item(),
                }

        orig_loop = trainer_ft.loop
        try:
            trainer_ft.loop = types.MethodType(finetune_loop, trainer_ft)
            optimizer = torch.optim.Adam(
                list(self.student.parameters()),
                lr=self.lr,
            )
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer, T_max=self.epochs
            )
            trainer_ft.scheduler = scheduler
            best_metrics = trainer_ft.fit(
                model=self.student,
                epochs=self.epochs,
                optimizer=optimizer,
                train_dataloader=self.data_loader(pub, "train"),
                val_dataloader=self.data_loader(pub, "val"),
                test_dataloader=self.data_loader(pub, "test"),
                checkpoint=True,
                global_weights=None,
                prefix="finetune/",
            )
            self.info["finetune_best_metrics"] = best_metrics
        finally:
            trainer_ft.loop = orig_loop

    def _prepare_private_public(self):
        # Try load cached
        if os.path.exists(self.private_data_path) and os.path.exists(
            self.public_data_path
        ):
            try:
                self.private_data = torch.load(self.private_data_path)
                self.public_data = torch.load(
                    self.public_data_path).to(self.device)
                return
            except Exception as e:
                self.logger.warning(f"Load private/public cache failed: {e}")

        # Build and cache
        self.logger.info("Caching private/public split to disk")
        private_data, public_data = split_graph_private_public_pyg(
            self.data,
            privacy_ratio=self.privacy_ratio,
            public_split=(0.7, 0.15, 0.15),
            device=self.device,
        )
        self.private_data = private_data
        self.public_data = public_data
        os.makedirs(self.cache_dir, exist_ok=True)
        try:
            torch.save(self.private_data, self.private_data_path)
            torch.save(self.public_data, self.public_data_path)
            self.logger.info("Split saved: private_data.pt, public_data.pt")
        except Exception as e:
            self.logger.warning(f"Failed to save cached splits: {e}")

    def _compute_node_stats(self, pub: Data) -> Tensor:
        # ä½¿ç”¨ partitioner çš„ _degree_vector æ–¹æ³•
        deg = self.partitioner._degree_vector(pub).float().to(self.device)
        # è®¡ç®— log(1+deg_pub(v))ï¼Œä¸æ–‡æ¡£ä¸€è‡´
        log_deg = torch.log(1 + deg + 1e-8)

        # è®¡ç®—ç‰¹å¾èŒƒæ•° ||x_v||_2ï¼ˆåŸå§‹å€¼ï¼Œä¸å½’ä¸€åŒ–ï¼‰
        feats = pub.x.to(self.device)
        feat_norm = torch.norm(feats, dim=1)

        # è¿”å› [log(1+deg), ||x_v||_2]
        return torch.stack([log_deg, feat_norm], dim=1)

    def _compute_bucket_id(self, deg_pub: Tensor, feat_norm: Tensor, tau_deg: float, tau_fea: float) -> Tensor:
        """
        è®¡ç®—èŠ‚ç‚¹çš„æ¡¶ç¼–å·

        Args:
            deg_pub: å…¬å…±å›¾ä¸Šçš„åº¦æ•° [N]
            feat_norm: ç‰¹å¾èŒƒæ•° [N]
            tau_deg: åº¦æ•°çš„ä¸­ä½æ•°é˜ˆå€¼
            tau_fea: ç‰¹å¾å¼ºåº¦çš„ä¸­ä½æ•°é˜ˆå€¼

        Returns:
            bucket_ids: æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”çš„æ¡¶ç¼–å· [N]ï¼Œæ¯ä¸ªå…ƒç´ å€¼ä¸º {1, 2, 3, 4} ä¹‹ä¸€ã€‚
                        æ¡¶ç¼–å·æ ¹æ®å…¬å¼ k(v)=1 + ğŸ™[g_sa(v)â‰¥Ï„_deg] + 2Â·ğŸ™[g_fs(v)â‰¥Ï„_fea] è‡ªåŠ¨è®¡ç®—
        """
        # è®¡ç®—ç»“æ„æ´»è·ƒåº¦: g_sa(v) = log(1 + deg_pub(v))
        g_sa = torch.log(1 + deg_pub + 1e-8)
        # è®¡ç®—ç‰¹å¾å¼ºåº¦: g_fs(v) = ||x_v||_2
        g_fs = feat_norm

        # æ¡¶ç¼–å·å‡½æ•°: k(v) = 1 + ğŸ™[g_sa(v)â‰¥Ï„_deg] + 2Â·ğŸ™[g_fs(v)â‰¥Ï„_fea]
        bucket_ids = 1 + (g_sa >= tau_deg).long() + \
            2 * (g_fs >= tau_fea).long()
        return bucket_ids

    def _compute_bucket_conditioned_tsv(
        self,
        public_data: Data,
        teachers: List[TeacherModule],
        num_buckets: int = 4,
        anchor_ratio: float = 0.1,
        logger=None,
    ) -> tuple[Tensor, dict]:
        """
        è®¡ç®—æ¡¶æ¡ä»¶åŒ–TSVï¼ˆTeacher Specificity Vectorï¼‰

        æ ¹æ®æ–‡æ¡£NADES.MDçš„æè¿°ï¼š
        1. é€‰æ‹©å…¬å…±é”šç‚¹é›† V_A âŠ‚ V_pub
        2. åŸºäºç»“æ„æ´»è·ƒåº¦å’Œç‰¹å¾å¼ºåº¦å¯¹é”šç‚¹åˆ†æ¡¶ï¼ˆ2Ã—2ï¼Œå¾—åˆ°4ä¸ªæ¡¶ï¼‰
        3. å¯¹æ¯ä¸ªæ•™å¸ˆã€æ¯ä¸ªæ¡¶ï¼ŒåŸºäºæ•™å¸ˆå¯¹é”šç‚¹çš„é¢„æµ‹è®¡ç®—ç»Ÿè®¡é‡

        Args:
            public_data: å…¬å…±å›¾æ•°æ®
            teachers: æ•™å¸ˆæ¨¡å‹åˆ—è¡¨
            num_buckets: æ¡¶æ•°é‡ï¼ˆé»˜è®¤4ï¼‰
            anchor_ratio: é”šç‚¹æ¯”ä¾‹ï¼ˆé»˜è®¤0.1ï¼Œå³10%ï¼‰
            logger: æ—¥å¿—è®°å½•å™¨

        Returns:
            tsv: TSVå¼ é‡ [num_teachers, num_buckets, 6]
            bucket_thresholds: åˆ†æ¡¶é˜ˆå€¼å­—å…¸ {tau_deg, tau_fea}
        """
        device = self.device
        num_teachers = len(teachers)

        # 1. é€‰æ‹©å…¬å…±é”šç‚¹é›† V_A âŠ‚ V_pub
        num_anchor_nodes = max(1, int(public_data.num_nodes * anchor_ratio))
        # éšæœºé€‰æ‹©é”šç‚¹ï¼ˆä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»–ç­–ç•¥ï¼Œå¦‚å‡åŒ€é‡‡æ ·ï¼‰
        anchor_indices = torch.randperm(public_data.num_nodes, device=device)[
            :num_anchor_nodes]
        anchor_indices = anchor_indices.sort()[0]  # æ’åºä»¥ä¾¿åç»­ä½¿ç”¨

        if logger:
            logger.info(f"é€‰æ‹© {num_anchor_nodes} ä¸ªå…¬å…±é”šç‚¹ç”¨äºTSVè®¡ç®—")

        # 2. è®¡ç®—é”šç‚¹çš„å…¬å…±è¯­å¢ƒé‡
        deg_pub = self.partitioner._degree_vector(
            public_data).float().to(device)
        feat_norm = torch.norm(public_data.x, dim=1).to(device)

        anchor_deg = deg_pub[anchor_indices]
        anchor_feat_norm = feat_norm[anchor_indices]

        # è®¡ç®—ç»“æ„æ´»è·ƒåº¦: g_sa(v) = log(1 + deg_pub(v))
        g_sa_anchor = torch.log(1 + anchor_deg + 1e-8)
        # è®¡ç®—ç‰¹å¾å¼ºåº¦: g_fs(v) = ||x_v||_2
        g_fs_anchor = anchor_feat_norm

        # 3. è®¡ç®—ä¸­ä½æ•°é˜ˆå€¼
        tau_deg = torch.median(g_sa_anchor).item()
        tau_fea = torch.median(g_fs_anchor).item()

        bucket_thresholds = {"tau_deg": tau_deg, "tau_fea": tau_fea}

        if logger:
            logger.info(f"åˆ†æ¡¶é˜ˆå€¼: tau_deg={tau_deg:.4f}, tau_fea={tau_fea:.4f}")

        # 4. å¯¹é”šç‚¹åˆ†æ¡¶
        anchor_bucket_ids = self._compute_bucket_id(
            anchor_deg, anchor_feat_norm, tau_deg, tau_fea
        )

        # ç»Ÿè®¡æ¯ä¸ªæ¡¶çš„é”šç‚¹æ•°é‡
        bucket_counts = {}
        for b in range(1, num_buckets + 1):
            bucket_counts[b] = (anchor_bucket_ids == b).sum().item()
            if logger:
                logger.info(f"æ¡¶ {b} åŒ…å« {bucket_counts[b]} ä¸ªé”šç‚¹")

        # 5. è·å–æ•™å¸ˆå¯¹é”šç‚¹çš„é¢„æµ‹
        anchor_x = public_data.x[anchor_indices].to(device)
        anchor_edge_index = public_data.edge_index.to(device)

        teacher_preds_anchor = []  # æ¯ä¸ªæ•™å¸ˆçš„é¢„æµ‹ [num_anchor, num_classes]
        teacher_logits_anchor = []  # æ¯ä¸ªæ•™å¸ˆçš„logits [num_anchor, num_classes]

        for teacher in teachers:
            teacher.eval()
            with torch.no_grad():
                # æ³¨æ„ï¼šæ•™å¸ˆæ¨¡å‹éœ€è¦åœ¨æ•´ä¸ªå…¬å…±å›¾ä¸Šè¿›è¡Œé¢„æµ‹ï¼ˆå› ä¸ºå›¾ç»“æ„ï¼‰
                # ä½†æˆ‘ä»¬åªå…³å¿ƒé”šç‚¹çš„é¢„æµ‹
                logits_all = teacher(public_data.x.to(
                    device), public_data.edge_index.to(device))
                logits_anchor = logits_all[anchor_indices]
                probs_anchor = F.softmax(logits_anchor, dim=-1)

                teacher_logits_anchor.append(logits_anchor)
                teacher_preds_anchor.append(probs_anchor)

        # [num_teachers, num_anchor, num_classes]
        teacher_preds_anchor = torch.stack(teacher_preds_anchor, dim=0)
        # [num_teachers, num_anchor, num_classes]
        teacher_logits_anchor = torch.stack(teacher_logits_anchor, dim=0)

        # 6. å¯¹æ¯ä¸ªæ•™å¸ˆã€æ¯ä¸ªæ¡¶è®¡ç®—ç»Ÿè®¡é‡
        tsv_list = []

        for teacher_idx in range(num_teachers):
            teacher_tsv_buckets = []

            for bucket_id in range(1, num_buckets + 1):
                # è·å–è¯¥æ¡¶çš„é”šç‚¹ç´¢å¼•
                bucket_mask = (anchor_bucket_ids == bucket_id)
                bucket_anchor_indices_local = torch.where(bucket_mask)[0]

                if len(bucket_anchor_indices_local) == 0:
                    # å¦‚æœæ¡¶ä¸ºç©ºï¼Œä½¿ç”¨é›¶å‘é‡
                    teacher_tsv_buckets.append(torch.zeros(6, device=device))
                    if logger:
                        logger.warning(
                            f"æ•™å¸ˆ {teacher_idx}, æ¡¶ {bucket_id} ä¸ºç©ºï¼Œä½¿ç”¨é›¶å‘é‡")
                    continue

                # è·å–è¯¥æ¡¶å†…é”šç‚¹çš„é¢„æµ‹
                # [|V_A^b|, num_classes]
                preds_bucket = teacher_preds_anchor[teacher_idx,
                                                    bucket_anchor_indices_local]
                # [|V_A^b|, num_classes]
                logits_bucket = teacher_logits_anchor[teacher_idx,
                                                      bucket_anchor_indices_local]

                # è®¡ç®—top-1å’Œtop-2æ¦‚ç‡å’Œlogits
                top2_probs, top2_indices = torch.topk(
                    preds_bucket, k=2, dim=1)  # [|V_A^b|, 2]
                p_top1 = top2_probs[:, 0]  # [|V_A^b|]
                p_top2 = top2_probs[:, 1]  # [|V_A^b|]

                # è·å–å¯¹åº”çš„logits
                batch_indices = torch.arange(
                    len(bucket_anchor_indices_local), device=device)
                z_top1 = logits_bucket[batch_indices,
                                       top2_indices[:, 0]]  # [|V_A^b|]
                z_top2 = logits_bucket[batch_indices,
                                       top2_indices[:, 1]]  # [|V_A^b|]

                # è®¡ç®—ç»Ÿè®¡é‡
                # 1. å¹³å‡ç†µ: Î¼_H^(i,b) = (1/|V_A^b|) Î£_{aâˆˆV_A^b} H(p_a^(i))
                entropy = -torch.sum(preds_bucket *
                                     torch.log(preds_bucket + 1e-8), dim=1)
                mu_H = entropy.mean()

                # 2. å¹³å‡æ¦‚ç‡é—´éš”: Î¼_M^(i,b) = (1/|V_A^b|) Î£_{aâˆˆV_A^b} (p_{a,1}^(i) - p_{a,2}^(i))
                mu_M = (p_top1 - p_top2).mean()

                # 3. top-1æ¦‚ç‡åˆ†ä½æ•°: (q_0.25, q_0.5, q_0.75)
                q_25 = torch.quantile(p_top1, 0.25)
                q_50 = torch.quantile(p_top1, 0.50)
                q_75 = torch.quantile(p_top1, 0.75)

                # 4. æ¸©åº¦å°ºåº¦proxy: Î¼_tp^(i,b) = (1/|V_A^b|) Î£_{aâˆˆV_A^b} log(1+exp(z_{a,1}^(i)-z_{a,2}^(i)))
                mu_tp = torch.log(1 + torch.exp(z_top1 - z_top2) + 1e-8).mean()

                # æ‹¼æ¥TSV: [Î¼_H, Î¼_M, q_0.25, q_0.5, q_0.75, Î¼_tp]
                tsv_bucket = torch.stack([mu_H, mu_M, q_25, q_50, q_75, mu_tp])
                teacher_tsv_buckets.append(tsv_bucket)

            # [num_buckets, 6]
            tsv_list.append(torch.stack(teacher_tsv_buckets, dim=0))

        # æœ€ç»ˆTSVå½¢çŠ¶: [num_teachers, num_buckets, 6]
        tsv = torch.stack(tsv_list, dim=0).to(torch.float32)

        if logger:
            logger.info(f"TSVè®¡ç®—å®Œæˆ: shape={tuple(tsv.shape)}")

        return tsv, bucket_thresholds

    def _predict_public_by_teachers(
        self, pub: Data, teachers: List[TeacherModule]
    ) -> Tensor:
        logits = []
        x = pub.x.to(self.device)
        e = pub.edge_index.to(self.device)
        for t in teachers:
            t.eval()
            with torch.no_grad():
                teacher_logits = t(x, e)
                logits.append(teacher_logits)
        return torch.stack(logits, dim=1)

    def _query_pseudo_labels_random_abte(self, pub: Data) -> dict:
        """
        ä¸€æ¬¡æ€§éšæœºé€‰æ‹©self.num_queriesä¸ªèŠ‚ç‚¹ï¼Œæ‰¹é‡è®¡ç®—ABTEé¢„æµ‹ï¼Œç„¶åå¯¹æ¯ä¸ªèŠ‚ç‚¹åˆ†åˆ«åŠ å™ªï¼Œä½¿ç”¨RDPæ ¸ç®—éšç§æˆæœ¬å¹¶æ±‡æ€»æ€»æˆæœ¬
        ä»…ä»æœ‰æ ‡ç­¾çš„è®­ç»ƒèŠ‚ç‚¹ä¸­é€‰æ‹©ï¼ˆæ ‡ç­¾ä¸º0æˆ–1ï¼‰ï¼Œæ’é™¤æœªæ ‡ç­¾èŠ‚ç‚¹ï¼ˆæ ‡ç­¾ä¸º-1ï¼Œå¦‚ellipticæ•°æ®é›†ï¼‰
        """

        # è·å–è®­ç»ƒèŠ‚ç‚¹ç´¢å¼•
        train_node_indices = pub.train_mask.nonzero(as_tuple=True)[
            0].to(self.device)

        # è·å–è®­ç»ƒèŠ‚ç‚¹çš„æ ‡ç­¾
        train_labels = pub.y[train_node_indices].to(self.device)

        # æ£€æŸ¥æ˜¯å¦æœ‰æœªæ ‡ç­¾èŠ‚ç‚¹ï¼ˆæ ‡ç­¾ä¸º-1ï¼‰
        # å…¼å®¹ä»¥å‰çš„æ•°æ®é›†ï¼šå¦‚æœæœ€å°æ ‡ç­¾å€¼ >= 0ï¼Œè¯´æ˜æ‰€æœ‰è®­ç»ƒèŠ‚ç‚¹éƒ½æ˜¯æœ‰æ ‡ç­¾çš„ï¼ˆæ ‡ç­¾ä¸º0æˆ–1ï¼‰
        min_label = train_labels.min().item()
        if min_label < 0:
            # æœ‰æœªæ ‡ç­¾èŠ‚ç‚¹ï¼Œåªé€‰æ‹©æ ‡ç­¾ä¸º0æˆ–1çš„èŠ‚ç‚¹
            labeled_mask = train_labels >= 0  # æ ‡ç­¾ä¸º0æˆ–1çš„èŠ‚ç‚¹ï¼ˆæ’é™¤-1ï¼‰
            labeled_train_indices = train_node_indices[labeled_mask]
            self.logger.info(
                f"è®­ç»ƒèŠ‚ç‚¹æ€»æ•°: {len(train_node_indices)}, "
                f"æœ‰æ ‡ç­¾èŠ‚ç‚¹æ•°: {len(labeled_train_indices)} (æ ‡ç­¾ä¸º0æˆ–1), "
                f"æœªæ ‡ç­¾èŠ‚ç‚¹æ•°: {len(train_node_indices) - len(labeled_train_indices)} (æ ‡ç­¾ä¸º-1)"
            )
        else:
            # å…¼å®¹ä»¥å‰çš„æ•°æ®é›†ï¼šåªæœ‰ä¸¤ç±»æ ‡ç­¾ï¼Œæ‰€æœ‰è®­ç»ƒèŠ‚ç‚¹éƒ½æ˜¯æœ‰æ ‡ç­¾çš„
            labeled_train_indices = train_node_indices
            self.logger.info(
                f"ä»è®­ç»ƒèŠ‚ç‚¹ä¸­{len(train_node_indices)}ä¸ªèŠ‚ç‚¹ä¸­ä¸€æ¬¡æ€§éšæœºæŸ¥è¯¢ {self.num_queries} ä¸ªèŠ‚ç‚¹"
            )

        # ç¡®å®šè¦æŸ¥è¯¢çš„èŠ‚ç‚¹æ•°é‡ï¼ˆä»æœ‰æ ‡ç­¾çš„èŠ‚ç‚¹ä¸­é€‰æ‹©ï¼‰
        num_nodes_to_query = min(self.num_queries, len(labeled_train_indices))
        if num_nodes_to_query < self.num_queries:
            self.logger.warning(
                f"æœ‰æ ‡ç­¾çš„è®­ç»ƒèŠ‚ç‚¹æ•°é‡ ({len(labeled_train_indices)}) å°äºæŸ¥è¯¢æ•°é‡ ({self.num_queries})ï¼Œ"
                f"å°†æŸ¥è¯¢æ‰€æœ‰ {num_nodes_to_query} ä¸ªæœ‰æ ‡ç­¾çš„è®­ç»ƒèŠ‚ç‚¹"
            )

        # 1. ä¸€æ¬¡æ€§éšæœºé€‰æ‹©self.num_queriesä¸ªèŠ‚ç‚¹ï¼ˆä»…ä»æœ‰æ ‡ç­¾çš„èŠ‚ç‚¹ä¸­é€‰æ‹©ï¼‰
        if num_nodes_to_query < len(labeled_train_indices):
            # éšæœºé‡‡æ ·
            perm = torch.randperm(
                len(labeled_train_indices), device=self.device)
            selected_indices_local = perm[:num_nodes_to_query]
            selected_indices = labeled_train_indices[
                selected_indices_local
            ]  # å…¨å±€èŠ‚ç‚¹ç´¢å¼•
        else:
            # å¦‚æœèŠ‚ç‚¹æ•°ä¸å¤Ÿï¼Œä½¿ç”¨æ‰€æœ‰æœ‰æ ‡ç­¾çš„èŠ‚ç‚¹
            selected_indices = labeled_train_indices

        # å­¦ç”Ÿæ¨¡å‹ä¸ABTEæ¨¡å—è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.student.eval()
        if hasattr(self, "attn") and self.attn is not None:
            self.attn.eval()

        # è®¡ç®—æ‰€æœ‰è®­ç»ƒèŠ‚ç‚¹çš„èŠ‚ç‚¹ç»Ÿè®¡

        # 2. æ‰¹é‡è·å–å­¦ç”Ÿæ¨¡å‹å¯¹é€‰ä¸­èŠ‚ç‚¹çš„é¢„æµ‹å’ŒåµŒå…¥
        with torch.no_grad():
            _, h_v_all = self.student(
                pub.x.to(self.device),
                pub.edge_index.to(self.device),
                return_intermediate=True,
            )
            # æ‰¹é‡å–é€‰ä¸­çš„èŠ‚ç‚¹
            # [num_nodes_to_query, hidden_dim]
            selected_h_v = h_v_all[selected_indices]
            selected_node_stats = self.node_stats_cache[
                selected_indices
            ]  # [num_nodes_to_query, node_stats_dim]
            # è·å–æ•™å¸ˆé¢„æµ‹
            selected_teacher_preds = self.teacher_preds_cache[selected_indices].to(
                self.device
            )  # [num_nodes_to_query, S, num_classes]

            # è®¡ç®—é€‰ä¸­èŠ‚ç‚¹çš„æ¡¶ç¼–å·ï¼ˆå¦‚æœä½¿ç”¨æ¡¶æ¡ä»¶åŒ–TSVï¼‰
            bucket_ids = None
            if self.bucket_thresholds is not None and self.tsv.dim() == 3:
                # è®¡ç®—é€‰ä¸­èŠ‚ç‚¹çš„åº¦æ•°å’Œç‰¹å¾èŒƒæ•°
                deg_pub = self.partitioner._degree_vector(
                    self.public_data).float().to(self.device)
                feat_norm = torch.norm(
                    self.public_data.x, dim=1).to(self.device)

                selected_deg = deg_pub[selected_indices]
                selected_feat_norm = feat_norm[selected_indices]

                # è®¡ç®—æ¡¶ç¼–å·
                bucket_ids = self._compute_bucket_id(
                    selected_deg,
                    selected_feat_norm,
                    self.bucket_thresholds["tau_deg"],
                    self.bucket_thresholds["tau_fea"],
                ).to(self.device)

            # ä½¿ç”¨Aggregatorçš„æ‰¹é‡æŸ¥è¯¢æ–¹æ³•è¿›è¡ŒABTEèšåˆ
            agg_logits_selected, _ = self.aggregator.batch_query_with_abte(
                selected_h_v,
                selected_node_stats,
                selected_teacher_preds,
                bucket_ids=bucket_ids,
            )  # [num_nodes_to_query, num_classes]

            # è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆè½¯æ ‡ç­¾ï¼‰
            agg_soft_labels = F.softmax(
                agg_logits_selected, dim=-1
            )  # [num_nodes_to_query, num_classes]

        # 3. æ‰¹é‡ä½¿ç”¨Dirichletæœºåˆ¶åŠ å™ª
        self.logger.info("å¯¹é€‰ä¸­çš„èŠ‚ç‚¹è¿›è¡ŒDirichletæœºåˆ¶åŠ å™ª")
        noisy_labels = self.aggregator.batch_add_noise(
            agg_soft_labels
        )  # [num_nodes_to_query, num_classes]

        # ä¿å­˜ä¼ªæ ‡ç­¾å’ŒåŠ å™ªå‰çš„è½¯æ ‡ç­¾
        pseudo_labels_dict = {}
        pre_soft_labels_dict = {}
        for i, node_idx in enumerate(selected_indices):
            node_idx_item = node_idx.item()
            pseudo_labels_dict[node_idx_item] = noisy_labels[i].detach().cpu()
            pre_soft_labels_dict[node_idx_item] = agg_soft_labels[i].detach(
            ).cpu()
        # 4. ä½¿ç”¨Aggregatorçš„RDPæ ¸ç®—æ–¹æ³•
        num_queries = len(pseudo_labels_dict)
        if num_queries > 0:
            rdp_result = self.aggregator.compute_total_rdp_cost(num_queries)
            self.epsilon_final = rdp_result["epsilon_final_rdp"]
            self.info["eta"] = self.eta
            self.info["epsilon_final"] = round(self.epsilon_final, 2).item()
            self.info["num_queries"] = self.num_queries

            if "error" in rdp_result:
                self.logger.warning(f"RDPæ ¸ç®—è®¡ç®—å¤±è´¥: {rdp_result['error']}")
                self.logger.info("RDPéšç§æˆæœ¬æ ¸ç®—:")
                self.logger.info(f"  æ€»æŸ¥è¯¢æ¬¡æ•°: {num_queries}")
            else:
                self.logger.info("ä½¿ç”¨RDPæ ¸ç®—æ–¹æ³•è®¡ç®—éšç§æˆæœ¬:")
                self.logger.info(
                    f"  æ€»æŸ¥è¯¢æ¬¡æ•°: {rdp_result['total_queries']}ï¼Œæ¯æ¬¡æŸ¥è¯¢eta: {rdp_result['eta']:.2f}ï¼Œ"
                    f"Delta_1_F: {rdp_result['Delta_1_F']:.2f}ï¼Œgamma: {rdp_result['gamma']:.2f}"
                )
                self.logger.info(
                    f"  RDPæ ¸ç®—åçš„æœ€ç»ˆepsilon (delta={rdp_result['delta']}): {rdp_result['epsilon_final_rdp']:.2f}ï¼Œ"
                    f"æœ€ä¼˜RDPé˜¶æ•°alpha: {rdp_result['best_alpha']:.2f}ï¼Œ"
                    f"æœ‰æ•ˆalphaé˜¶æ•°: {rdp_result['valid_alphas_count']}/{rdp_result['total_alpha_count']}"
                )
                if rdp_result["valid_alphas_count"] < rdp_result["total_alpha_count"]:
                    skipped_count = (
                        rdp_result["total_alpha_count"]
                        - rdp_result["valid_alphas_count"]
                    )
                    self.logger.warning(
                        f"  {skipped_count}ä¸ªalphaé˜¶è¿”å›infï¼ˆå¯èƒ½ <= alpha * Delta_1_Fï¼‰ï¼Œ"
                        f"è¿™äº›alphaé˜¶å·²è¢«è·³è¿‡"
                    )

                # ä¿å­˜RDPæ ¸ç®—ç»“æœ
                self.rdp_accounting_result = rdp_result
        else:
            self.logger.warning("æœªç”Ÿæˆä»»ä½•ä¼ªæ ‡ç­¾ï¼Œæ— æ³•è¿›è¡ŒRDPæ ¸ç®—")

        # ç»Ÿè®¡åŠ å™ªå‰çš„è½¯æ ‡ç­¾åˆ†å¸ƒ
        if len(pre_soft_labels_dict) > 0:
            fraud_count_pre = 0
            normal_count_pre = 0
            fraud_probs_pre = []
            normal_probs_pre = []
            for node_idx, soft_v in pre_soft_labels_dict.items():
                if isinstance(soft_v, torch.Tensor):
                    fraud_class_idx = min(
                        self.fraud_class_idx, len(soft_v) - 1)
                    fraud_prob = soft_v[fraud_class_idx].item()
                    if fraud_prob > 0.5:
                        fraud_count_pre += 1
                        fraud_probs_pre.append(fraud_prob)
                    else:
                        normal_count_pre += 1
                        normal_probs_pre.append(fraud_prob)
            self.info["åŠ å™ªå‰åå‘æ¬ºè¯ˆ"] = fraud_count_pre
            total_pre = len(pre_soft_labels_dict)
            self.logger.info("åŠ å™ªå‰èšåˆå™¨è½¯æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡:")
            self.logger.info(
                f"  åå‘æ¬ºè¯ˆ: {fraud_count_pre} ä¸ª ({fraud_count_pre / total_pre * 100:.2f}%)åå‘æ­£å¸¸: {normal_count_pre} ä¸ª ({normal_count_pre / total_pre * 100:.2f}%)"
            )
            if len(fraud_probs_pre) > 0:
                self.logger.info(
                    f"  æ¬ºè¯ˆæ ‡ç­¾æ¦‚ç‡: å‡å€¼={sum(fraud_probs_pre) / len(fraud_probs_pre):.4f}, "
                    f"æœ€å¤§={max(fraud_probs_pre):.4f}, æœ€å°={min(fraud_probs_pre):.4f}"
                )
            if len(normal_probs_pre) > 0:
                self.logger.info(
                    f"  æ­£å¸¸æ ‡ç­¾æ¦‚ç‡: å‡å€¼={sum(normal_probs_pre) / len(normal_probs_pre):.4f}, "
                    f"æœ€å¤§={max(normal_probs_pre):.4f}, æœ€å°={min(normal_probs_pre):.4f}"
                )

        # ç»Ÿè®¡ä¼ªæ ‡ç­¾åˆ†å¸ƒï¼ˆåŠ å™ªåï¼‰
        if len(pseudo_labels_dict) > 0:
            fraud_count = 0
            normal_count = 0
            fraud_probs = []
            normal_probs = []

            for node_idx, pseudo_label in pseudo_labels_dict.items():
                pseudo_label_tensor = pseudo_label
                if isinstance(pseudo_label_tensor, torch.Tensor):
                    fraud_class_idx = min(
                        self.fraud_class_idx, len(pseudo_label_tensor) - 1
                    )
                    fraud_prob = pseudo_label_tensor[fraud_class_idx].item()

                    if fraud_prob > 0.5:
                        fraud_count += 1
                        fraud_probs.append(fraud_prob)
                    else:
                        normal_count += 1
                        normal_probs.append(fraud_prob)
            self.info["åŠ å™ªååå‘æ¬ºè¯ˆ"] = fraud_count
            self.logger.info("åŠ å™ªåä¼ªæ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡:")
            self.logger.info(
                f"  åå‘æ¬ºè¯ˆ: {fraud_count} ä¸ª ({fraud_count / len(pseudo_labels_dict) * 100:.2f}%)åå‘æ­£å¸¸: {normal_count} ä¸ª ({normal_count / len(pseudo_labels_dict) * 100:.2f}%)"
            )

            if len(fraud_probs) > 0:
                fraud_prob_mean = sum(fraud_probs) / len(fraud_probs)
                fraud_prob_max = max(fraud_probs)
                fraud_prob_min = min(fraud_probs)
                self.logger.info(
                    f"  æ¬ºè¯ˆæ ‡ç­¾æ¦‚ç‡ç»Ÿè®¡: å‡å€¼={fraud_prob_mean:.4f}, "
                    f"æœ€å¤§={fraud_prob_max:.4f}, æœ€å°={fraud_prob_min:.4f}"
                )

            if len(normal_probs) > 0:
                normal_prob_mean = sum(normal_probs) / len(normal_probs)
                normal_prob_max = max(normal_probs)
                normal_prob_min = min(normal_probs)
                self.logger.info(
                    f"  æ­£å¸¸æ ‡ç­¾æ¦‚ç‡ç»Ÿè®¡: å‡å€¼={normal_prob_mean:.4f}, "
                    f"æœ€å¤§={normal_prob_max:.4f}, æœ€å°={normal_prob_min:.4f}"
                )

        # ä¿å­˜åˆ°å¯¹è±¡ï¼Œä¾¿äºå…¶ä»–ä½ç½®ä½¿ç”¨
        self.pre_soft_labels_dict = pre_soft_labels_dict

        # å°†ä¼ªæ ‡ç­¾è½¬æ¢ä¸ºtensorå¹¶å­˜å‚¨åˆ°public_dataä¸­
        pseudo_labels_tensor = torch.zeros(
            (pub.num_nodes, self.out_channels),
            device=self.device,
            dtype=torch.float32,
        )
        has_pseudo_label = torch.zeros(
            pub.num_nodes, dtype=torch.bool, device=self.device
        )
        for node_idx, pseudo_label in pseudo_labels_dict.items():
            pseudo_labels_tensor[node_idx] = pseudo_label.to(self.device)
            has_pseudo_label[node_idx] = True
        pub.pseudo_labels = pseudo_labels_tensor
        pub.has_pseudo_label = has_pseudo_label
        self.logger.info(
            f"ä¼ªæ ‡ç­¾å·²å­˜å‚¨åˆ°public_dataï¼Œå…± {has_pseudo_label.sum().item()} ä¸ªèŠ‚ç‚¹æœ‰ä¼ªæ ‡ç­¾"
        )

        return pseudo_labels_dict

    def _query_pseudo_labels_mean(self, pub: Data) -> dict:
        """
        æ¶ˆèå®éªŒï¼šä½¿ç”¨æ•™å¸ˆé¢„æµ‹çš„å¹³å‡å€¼ç”Ÿæˆä¼ªæ ‡ç­¾ï¼ˆä¸ä½¿ç”¨ABTEï¼‰
        ä¸€æ¬¡æ€§éšæœºé€‰æ‹©self.num_queriesä¸ªèŠ‚ç‚¹ï¼Œè®¡ç®—æ•™å¸ˆé¢„æµ‹çš„å¹³å‡å€¼ï¼Œç„¶åå¯¹æ¯ä¸ªèŠ‚ç‚¹åˆ†åˆ«åŠ å™ª
        ä»…ä»æœ‰æ ‡ç­¾çš„è®­ç»ƒèŠ‚ç‚¹ä¸­é€‰æ‹©ï¼ˆæ ‡ç­¾ä¸º0æˆ–1ï¼‰ï¼Œæ’é™¤æœªæ ‡ç­¾èŠ‚ç‚¹ï¼ˆæ ‡ç­¾ä¸º-1ï¼Œå¦‚ellipticæ•°æ®é›†ï¼‰
        """
        # è·å–è®­ç»ƒèŠ‚ç‚¹ç´¢å¼•
        train_node_indices = pub.train_mask.nonzero(as_tuple=True)[
            0].to(self.device)

        # è·å–è®­ç»ƒèŠ‚ç‚¹çš„æ ‡ç­¾
        train_labels = pub.y[train_node_indices].to(self.device)

        # æ£€æŸ¥æ˜¯å¦æœ‰æœªæ ‡ç­¾èŠ‚ç‚¹ï¼ˆæ ‡ç­¾ä¸º-1ï¼‰
        min_label = train_labels.min().item()
        if min_label < 0:
            # æœ‰æœªæ ‡ç­¾èŠ‚ç‚¹ï¼Œåªé€‰æ‹©æ ‡ç­¾ä¸º0æˆ–1çš„èŠ‚ç‚¹
            labeled_mask = train_labels >= 0
            labeled_train_indices = train_node_indices[labeled_mask]
            self.logger.info(
                f"è®­ç»ƒèŠ‚ç‚¹æ€»æ•°: {len(train_node_indices)}, "
                f"æœ‰æ ‡ç­¾èŠ‚ç‚¹æ•°: {len(labeled_train_indices)} (æ ‡ç­¾ä¸º0æˆ–1), "
                f"æœªæ ‡ç­¾èŠ‚ç‚¹æ•°: {len(train_node_indices) - len(labeled_train_indices)} (æ ‡ç­¾ä¸º-1)"
            )
        else:
            labeled_train_indices = train_node_indices
            self.logger.info(
                f"ä»è®­ç»ƒèŠ‚ç‚¹ä¸­{len(train_node_indices)}ä¸ªèŠ‚ç‚¹ä¸­ä¸€æ¬¡æ€§éšæœºæŸ¥è¯¢ {self.num_queries} ä¸ªèŠ‚ç‚¹"
            )

        # ç¡®å®šè¦æŸ¥è¯¢çš„èŠ‚ç‚¹æ•°é‡ï¼ˆä»æœ‰æ ‡ç­¾çš„èŠ‚ç‚¹ä¸­é€‰æ‹©ï¼‰
        num_nodes_to_query = min(self.num_queries, len(labeled_train_indices))
        if num_nodes_to_query < self.num_queries:
            self.logger.warning(
                f"æœ‰æ ‡ç­¾çš„è®­ç»ƒèŠ‚ç‚¹æ•°é‡ ({len(labeled_train_indices)}) å°äºæŸ¥è¯¢æ•°é‡ ({self.num_queries})ï¼Œ"
                f"å°†æŸ¥è¯¢æ‰€æœ‰ {num_nodes_to_query} ä¸ªæœ‰æ ‡ç­¾çš„è®­ç»ƒèŠ‚ç‚¹"
            )

        # 1. ä¸€æ¬¡æ€§éšæœºé€‰æ‹©self.num_queriesä¸ªèŠ‚ç‚¹ï¼ˆä»…ä»æœ‰æ ‡ç­¾çš„èŠ‚ç‚¹ä¸­é€‰æ‹©ï¼‰
        if num_nodes_to_query < len(labeled_train_indices):
            # éšæœºé‡‡æ ·
            perm = torch.randperm(
                len(labeled_train_indices), device=self.device)
            selected_indices_local = perm[:num_nodes_to_query]
            selected_indices = labeled_train_indices[
                selected_indices_local
            ]  # å…¨å±€èŠ‚ç‚¹ç´¢å¼•
        else:
            # å¦‚æœèŠ‚ç‚¹æ•°ä¸å¤Ÿï¼Œä½¿ç”¨æ‰€æœ‰æœ‰æ ‡ç­¾çš„èŠ‚ç‚¹
            selected_indices = labeled_train_indices

        # 2. è·å–æ•™å¸ˆé¢„æµ‹å¹¶è®¡ç®—å¹³å‡å€¼
        with torch.no_grad():
            # è·å–é€‰ä¸­èŠ‚ç‚¹çš„æ•™å¸ˆé¢„æµ‹ [num_nodes_to_query, S, num_classes]
            selected_teacher_preds = self.teacher_preds_cache[selected_indices].to(
                self.device
            )

            # è®¡ç®—æ•™å¸ˆé¢„æµ‹çš„å¹³å‡å€¼ [num_nodes_to_query, num_classes]
            # selected_teacher_preds: [num_nodes_to_query, S, num_classes]
            # mean along teacher dimension (dim=1)
            mean_soft_labels = selected_teacher_preds.mean(
                dim=1
            )  # [num_nodes_to_query, num_classes]

        # 3. æ‰¹é‡ä½¿ç”¨Dirichletæœºåˆ¶åŠ å™ª
        self.logger.info("å¯¹é€‰ä¸­çš„èŠ‚ç‚¹è¿›è¡ŒDirichletæœºåˆ¶åŠ å™ªï¼ˆåŸºäºæ•™å¸ˆå¹³å‡é¢„æµ‹ï¼‰")
        noisy_labels = self.aggregator.batch_add_noise(
            mean_soft_labels
        )  # [num_nodes_to_query, num_classes]

        # ä¿å­˜ä¼ªæ ‡ç­¾å’ŒåŠ å™ªå‰çš„è½¯æ ‡ç­¾
        pseudo_labels_dict = {}
        pre_soft_labels_dict = {}
        for i, node_idx in enumerate(selected_indices):
            node_idx_item = node_idx.item()
            pseudo_labels_dict[node_idx_item] = noisy_labels[i].detach().cpu()
            pre_soft_labels_dict[node_idx_item] = mean_soft_labels[i].detach(
            ).cpu()

        # 4. ä½¿ç”¨Aggregatorçš„RDPæ ¸ç®—æ–¹æ³•
        num_queries = len(pseudo_labels_dict)
        if num_queries > 0:
            rdp_result = self.aggregator.compute_total_rdp_cost(num_queries)
            self.epsilon_final = rdp_result["epsilon_final_rdp"]
            self.info["eta"] = self.eta
            self.info["epsilon_final"] = round(self.epsilon_final, 2).item()
            self.info["num_queries"] = self.num_queries

            if "error" in rdp_result:
                self.logger.warning(f"RDPæ ¸ç®—è®¡ç®—å¤±è´¥: {rdp_result['error']}")
                self.logger.info("RDPéšç§æˆæœ¬æ ¸ç®—:")
                self.logger.info(f"  æ€»æŸ¥è¯¢æ¬¡æ•°: {num_queries}")
            else:
                self.logger.info("ä½¿ç”¨RDPæ ¸ç®—æ–¹æ³•è®¡ç®—éšç§æˆæœ¬ï¼ˆåŸºäºæ•™å¸ˆå¹³å‡é¢„æµ‹ï¼‰:")
                self.logger.info(
                    f"  æ€»æŸ¥è¯¢æ¬¡æ•°: {rdp_result['total_queries']}ï¼Œæ¯æ¬¡æŸ¥è¯¢eta: {rdp_result['eta']:.2f}ï¼Œ"
                    f"Delta_1_F: {rdp_result['Delta_1_F']:.2f}ï¼Œgamma: {rdp_result['gamma']:.2f}"
                )
                self.logger.info(
                    f"  RDPæ ¸ç®—åçš„æœ€ç»ˆepsilon (delta={rdp_result['delta']}): {rdp_result['epsilon_final_rdp']:.2f}ï¼Œ"
                    f"æœ€ä¼˜RDPé˜¶æ•°alpha: {rdp_result['best_alpha']:.2f}ï¼Œ"
                    f"æœ‰æ•ˆalphaé˜¶æ•°: {rdp_result['valid_alphas_count']}/{rdp_result['total_alpha_count']}"
                )
                if rdp_result["valid_alphas_count"] < rdp_result["total_alpha_count"]:
                    skipped_count = (
                        rdp_result["total_alpha_count"]
                        - rdp_result["valid_alphas_count"]
                    )
                    self.logger.warning(
                        f"  {skipped_count}ä¸ªalphaé˜¶è¿”å›infï¼ˆå¯èƒ½ <= alpha * Delta_1_Fï¼‰ï¼Œ"
                        f"è¿™äº›alphaé˜¶å·²è¢«è·³è¿‡"
                    )

                # ä¿å­˜RDPæ ¸ç®—ç»“æœ
                self.rdp_accounting_result = rdp_result
        else:
            self.logger.warning("æœªç”Ÿæˆä»»ä½•ä¼ªæ ‡ç­¾ï¼Œæ— æ³•è¿›è¡ŒRDPæ ¸ç®—")

        # ç»Ÿè®¡åŠ å™ªå‰çš„è½¯æ ‡ç­¾åˆ†å¸ƒ
        if len(pre_soft_labels_dict) > 0:
            fraud_count_pre = 0
            normal_count_pre = 0
            fraud_probs_pre = []
            normal_probs_pre = []
            for node_idx, soft_v in pre_soft_labels_dict.items():
                if isinstance(soft_v, torch.Tensor):
                    fraud_class_idx = min(
                        self.fraud_class_idx, len(soft_v) - 1)
                    fraud_prob = soft_v[fraud_class_idx].item()
                    if fraud_prob > 0.5:
                        fraud_count_pre += 1
                        fraud_probs_pre.append(fraud_prob)
                    else:
                        normal_count_pre += 1
                        normal_probs_pre.append(fraud_prob)
            self.info["åŠ å™ªå‰åå‘æ¬ºè¯ˆ(mean)"] = fraud_count_pre
            total_pre = len(pre_soft_labels_dict)
            self.logger.info("åŠ å™ªå‰æ•™å¸ˆå¹³å‡é¢„æµ‹åˆ†å¸ƒç»Ÿè®¡:")
            self.logger.info(
                f"  åå‘æ¬ºè¯ˆ: {fraud_count_pre} ä¸ª ({fraud_count_pre / total_pre * 100:.2f}%)ï¼Œ"
                f"åå‘æ­£å¸¸: {normal_count_pre} ä¸ª ({normal_count_pre / total_pre * 100:.2f}%)"
            )
            if len(fraud_probs_pre) > 0:
                self.logger.info(
                    f"  æ¬ºè¯ˆæ ‡ç­¾æ¦‚ç‡: å‡å€¼={sum(fraud_probs_pre) / len(fraud_probs_pre):.4f}, "
                    f"æœ€å¤§={max(fraud_probs_pre):.4f}, æœ€å°={min(fraud_probs_pre):.4f}"
                )
            if len(normal_probs_pre) > 0:
                self.logger.info(
                    f"  æ­£å¸¸æ ‡ç­¾æ¦‚ç‡: å‡å€¼={sum(normal_probs_pre) / len(normal_probs_pre):.4f}, "
                    f"æœ€å¤§={max(normal_probs_pre):.4f}, æœ€å°={min(normal_probs_pre):.4f}"
                )

        # ç»Ÿè®¡ä¼ªæ ‡ç­¾åˆ†å¸ƒï¼ˆåŠ å™ªåï¼‰
        if len(pseudo_labels_dict) > 0:
            fraud_count = 0
            normal_count = 0
            fraud_probs = []
            normal_probs = []

            for node_idx, pseudo_label in pseudo_labels_dict.items():
                pseudo_label_tensor = pseudo_label
                if isinstance(pseudo_label_tensor, torch.Tensor):
                    fraud_class_idx = min(
                        self.fraud_class_idx, len(pseudo_label_tensor) - 1
                    )
                    fraud_prob = pseudo_label_tensor[fraud_class_idx].item()

                    if fraud_prob > 0.5:
                        fraud_count += 1
                        fraud_probs.append(fraud_prob)
                    else:
                        normal_count += 1
                        normal_probs.append(fraud_prob)
            self.info["åŠ å™ªååå‘æ¬ºè¯ˆ(mean)"] = fraud_count
            self.logger.info("åŠ å™ªåä¼ªæ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡ï¼ˆåŸºäºæ•™å¸ˆå¹³å‡é¢„æµ‹ï¼‰:")
            self.logger.info(
                f"  åå‘æ¬ºè¯ˆ: {fraud_count} ä¸ª ({fraud_count / len(pseudo_labels_dict) * 100:.2f}%)ï¼Œ"
                f"åå‘æ­£å¸¸: {normal_count} ä¸ª ({normal_count / len(pseudo_labels_dict) * 100:.2f}%)"
            )

            if len(fraud_probs) > 0:
                fraud_prob_mean = sum(fraud_probs) / len(fraud_probs)
                fraud_prob_max = max(fraud_probs)
                fraud_prob_min = min(fraud_probs)
                self.logger.info(
                    f"  æ¬ºè¯ˆæ ‡ç­¾æ¦‚ç‡ç»Ÿè®¡: å‡å€¼={fraud_prob_mean:.4f}, "
                    f"æœ€å¤§={fraud_prob_max:.4f}, æœ€å°={fraud_prob_min:.4f}"
                )

            if len(normal_probs) > 0:
                normal_prob_mean = sum(normal_probs) / len(normal_probs)
                normal_prob_max = max(normal_probs)
                normal_prob_min = min(normal_probs)
                self.logger.info(
                    f"  æ­£å¸¸æ ‡ç­¾æ¦‚ç‡ç»Ÿè®¡: å‡å€¼={normal_prob_mean:.4f}, "
                    f"æœ€å¤§={normal_prob_max:.4f}, æœ€å°={normal_prob_min:.4f}"
                )

        # ä¿å­˜åˆ°å¯¹è±¡ï¼Œä¾¿äºå…¶ä»–ä½ç½®ä½¿ç”¨
        self.pre_soft_labels_dict = pre_soft_labels_dict

        # å°†ä¼ªæ ‡ç­¾è½¬æ¢ä¸ºtensorå¹¶å­˜å‚¨åˆ°public_dataä¸­
        pseudo_labels_tensor = torch.zeros(
            (pub.num_nodes, self.out_channels),
            device=self.device,
            dtype=torch.float32,
        )
        has_pseudo_label = torch.zeros(
            pub.num_nodes, dtype=torch.bool, device=self.device
        )
        for node_idx, pseudo_label in pseudo_labels_dict.items():
            pseudo_labels_tensor[node_idx] = pseudo_label.to(self.device)
            has_pseudo_label[node_idx] = True
        pub.pseudo_labels = pseudo_labels_tensor
        pub.has_pseudo_label = has_pseudo_label
        self.logger.info(
            f"ä¼ªæ ‡ç­¾å·²å­˜å‚¨åˆ°public_dataï¼ˆåŸºäºæ•™å¸ˆå¹³å‡é¢„æµ‹ï¼‰ï¼Œå…± {has_pseudo_label.sum().item()} ä¸ªèŠ‚ç‚¹æœ‰ä¼ªæ ‡ç­¾"
        )

        return pseudo_labels_dict

    def _evaluate_abte_upper_bound(self):
        """
        ä½¿ç”¨ ABTE èšåˆå™¨åœ¨æ‰€æœ‰å…¬å¼€èŠ‚ç‚¹ä¸Šç”Ÿæˆæ— å™ªå£°ä¼ªæ ‡ç­¾ï¼Œ
        å¹¶ä¸å…¬å¼€æ•°æ®çš„çœŸå®æ ‡ç­¾å¯¹æ¯”ï¼Œè¯„ä¼°ç†è®ºä¸Šé™æ€§èƒ½ã€‚
        """

        if self.public_data is None:
            self.logger.warning("public_data æœªåˆå§‹åŒ–ï¼Œæ— æ³•è¯„ä¼° ABTE ç†è®ºä¸Šé™")
            return None

        if not hasattr(self, "aggregator") or self.aggregator is None:
            self.logger.warning("aggregator æœªåˆå§‹åŒ–ï¼Œæ— æ³•è¯„ä¼° ABTE ç†è®ºä¸Šé™")
            return None

        teacher_preds = getattr(self, "teacher_preds_cache", None)
        node_stats = getattr(self, "node_stats_cache", None)

        if teacher_preds is None or node_stats is None:
            self.logger.warning(
                "ç¼ºå°‘ teacher é¢„æµ‹æˆ–èŠ‚ç‚¹ç»Ÿè®¡ç¼“å­˜ï¼Œæ— æ³•è¯„ä¼° ABTE ç†è®ºä¸Šé™"
            )
            return None

        self.student.eval()
        if hasattr(self, "attn") and self.attn is not None:
            self.attn.eval()

        pub = self.public_data
        x = pub.x.to(self.device)
        edge_index = pub.edge_index.to(self.device)

        with torch.no_grad():
            _, node_embeddings = self.student(
                x,
                edge_index,
                return_intermediate=True,
            )

            agg_logits, _ = self.aggregator.batch_query_with_abte(
                node_embeddings,
                node_stats,
                teacher_preds.to(self.device),
            )

            agg_probs = F.softmax(agg_logits, dim=-1)

        labels = pub.y.detach().cpu()
        metrics = evaluate(labels, agg_logits.detach().cpu())

        self.logger.info("ABTE ç†è®ºä¸Šé™è¯„ä¼°ï¼ˆä½¿ç”¨å…¬å¼€æ ‡ç­¾ï¼Œæ— éšç§å™ªå£°ï¼‰: %s", metrics)

        pub.abte_upper_bound_logits = agg_logits.detach().cpu()
        pub.abte_upper_bound_probs = agg_probs.detach().cpu()
        self.abte_upper_bound_metrics = metrics

        return metrics
